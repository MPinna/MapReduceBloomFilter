{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Connect to remote host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from paramiko import SSHClient\n",
    "from scp import SCPClient\n",
    "\n",
    "USER='hadoop'\n",
    "HOST='172.16.4.135'\n",
    "PASSWD='hadoop'\n",
    "REMOTE_WORKING_DIR = 'bloom'\n",
    "REMOTE_PARAMS=f'/home/{USER}/{REMOTE_WORKING_DIR}/params'\n",
    "REMOTE_LOGS=f'/home/{USER}/{REMOTE_WORKING_DIR}/logs'\n",
    "PARAMS='data/params'\n",
    "LOGS='data/logs'\n",
    "\n",
    "client = SSHClient()\n",
    "client.load_system_host_keys()\n",
    "client.connect(hostname=HOST, username=USER, password=PASSWD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upload compute params script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPTS = ['scripts/compute_params.sh']\n",
    "scp = SCPClient(client.get_transport())\n",
    "scp.put(SCRIPTS, remote_path=REMOTE_WORKING_DIR)\n",
    "scp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download params from cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/', exist_ok=True)\n",
    "scp = SCPClient(client.get_transport())\n",
    "scp.get(REMOTE_PARAMS, recursive=True, local_path='data/')\n",
    "scp.get(REMOTE_LOGS, recursive=True, local_path='data/')\n",
    "scp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create hadoop input files (M,K,#Mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "import json\n",
    "\n",
    "folders = os.walk(PARAMS)\n",
    "os.makedirs('scripts/input/hadoop', exist_ok=True)\n",
    "\n",
    "INPUT='scripts/input/hadoop'\n",
    "\n",
    "with open('data/title.ratings.tsv') as f_in: \n",
    "    N = num_lines = sum(1 for line in f_in) - 1\n",
    "\n",
    "DEFAULT_MAPPERS = ceil(N / 8)\n",
    "n_mappers = [4, 6, 8, 10, 12] # 2, 1.33, 1, 0.8, 0.66 map/core\n",
    "n_splits = [ceil(N/m) for m in n_mappers]\n",
    "parameters = {}\n",
    "next(folders, None) # skip first element\n",
    "for folder in folders:  \n",
    "\n",
    "    tokens=folder[0].replace('\\\\','/').split('/')[-1].split('_')\n",
    "\n",
    "    P=float(tokens[1][1:])\n",
    "    K=int(tokens[2][1:])\n",
    "    if K == 1:\n",
    "        continue\n",
    "    params_file = f'{folder[0]}/{folder[2][0]}'\n",
    "\n",
    "    with(open(params_file)) as params:   \n",
    "        reader = csv.reader(params, delimiter=\"\\t\")\n",
    "        input = f'echo \"title.ratings.tsv $1 {DEFAULT_MAPPERS} '\n",
    "        input_filename = f'{tokens[1]}{tokens[2]}'\n",
    "        m_size = ''\n",
    "        k=0\n",
    "        with open(f'{INPUT}/{input_filename}', 'w') as input_file:\n",
    "            for line in reader:\n",
    "                m_size += f'{line[3]} '\n",
    "                k=line[4]\n",
    "            input += f'{m_size}{k} $2\"'\n",
    "            input_file.write(input)\n",
    "            parameters[f'P{P if P > 1e-05 else \"0.00001\"}K{K}'] = {'P': P, 'K': int(k), 'M': [int(m) for m in m_size.split(\" \")[0:-1]], 'MAP': 8}\n",
    "        if K == 0:\n",
    "            for n_mapper, n_split in zip(n_mappers, n_splits):\n",
    "                with open(f'{INPUT}/{input_filename}MAP{n_mapper}', 'w') as input_mapper_file:\n",
    "                    input_mapper = f'echo \"title.ratings.tsv $1 {n_split} {m_size}{k} $2\"'\n",
    "                    input_mapper_file.write(input_mapper)\n",
    "                    parameters[f'P{P if P > 1e-05 else \"0.00001\"}K{K}MAP{n_mapper}'] = {'P': P, 'K': int(k), 'M': [int(m) for m in m_size.split(\" \")[0:-1]], 'MAP': n_mapper}\n",
    "                    \n",
    "with open('data/parameters.json','w') as out:\n",
    "    json.dump(parameters, out, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create spark input files (M,K,#Mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "import json\n",
    "folders = os.walk(PARAMS)\n",
    "os.makedirs('scripts/input/spark', exist_ok=True)\n",
    "\n",
    "INPUT='scripts/input/spark'\n",
    "DEFAULT_PARTIONS=8\n",
    "\n",
    "n_mappers = [1, 2, 4, 6, 8, 10, 12, 14, 16] # 2, 1.33, 1, 0.8, 0.66 map/core\n",
    "\n",
    "next(folders, None) # skip first element\n",
    "for folder in folders:  \n",
    "\n",
    "    tokens=folder[0].replace('\\\\','/').split('/')[-1].split('_')\n",
    "\n",
    "    P=float(tokens[1][1:])\n",
    "    K=int(tokens[2][1:])\n",
    "    if K == 1:\n",
    "        continue\n",
    "    params_file = f'{folder[0]}/{folder[2][0]}'\n",
    "\n",
    "    parameters = {}\n",
    "    with(open(params_file)) as params:   \n",
    "        reader = csv.reader(params, delimiter=\"\\t\")\n",
    "        input = f'echo \"title.ratings.tsv $1 {DEFAULT_PARTIONS} '\n",
    "        input_filename = f'{tokens[1]}{tokens[2]}'\n",
    "        m_size = ''\n",
    "        k=0\n",
    "        with open(f'{INPUT}/{input_filename}', 'w') as input_file:\n",
    "            for line in reader:\n",
    "                m_size += f'{line[3]} '\n",
    "                k=line[4]\n",
    "            input += f'{m_size}{k} {P} $2\"'\n",
    "            input_file.write(input)\n",
    "\n",
    "        if K == 0:\n",
    "            for n_mapper in n_mappers:\n",
    "                with open(f'{INPUT}/{input_filename}MAP{n_mapper}', 'w') as input_mapper_file:\n",
    "                    input_mapper = f'echo \"title.ratings.tsv $1 {n_mapper} {m_size}{k} {P} $2\"'\n",
    "    \n",
    "                    input_mapper_file.write(input_mapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upload test and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_DIR = 'scripts/input'\n",
    "SCRIPTS = ['scripts/hadoop_test.sh','scripts/spark_test.sh']\n",
    "scp = SCPClient(client.get_transport())\n",
    "\n",
    "scp.put(TEST_DIR, recursive=True, remote_path=REMOTE_WORKING_DIR)\n",
    "scp.put(SCRIPTS, remote_path=REMOTE_WORKING_DIR)\n",
    "\n",
    "scp.close()\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8aaef33e4825cc75c10d5503a8badec4de06c5b884d15b4da0ae20ec31a339b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
